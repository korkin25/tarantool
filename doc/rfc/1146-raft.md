# Tarantool Raft

* **Status**: In progress
* **Start date**: 23-06-2019
* **Authors**: Vladislav Shpilevoy @Gerold103 \<v.shpilevoy@tarantool.org\>, Konstantin Osipov @kostja \<kostja@tarantool.org\>
* **Issues**: [#1146](https://github.com/tarantool/tarantool/issues/1146) [#3055](https://github.com/tarantool/tarantool/issues/3055) [#3234](https://github.com/tarantool/tarantool/issues/3234)

## Summary

Raft is a protocol to synchronously apply changes and make decisions on nodes of
a cluster.

The document describes design and implementation of Raft consensus protocol in
Tarantool.

At this moment it is not completed, still some open questions exist.

## Background and motivation

Raft is motivated by a long story about master promotion in Tarantool cluster.
Master promotion is a quite complex task, which consists of demotion of the
old master, synchronization of the nodes, and promotion of a new master. Despite
having so few steps, each of them is really complicated.

Master demotion should work even if the master is not available. Other nodes
should demote it by voting, and ignore all write-requests from it in case the
old master somehow returned.

Synchronization should be achieved in a major number of nodes, at least 50% + 1.

Master promotion should be done on one node only, obviously. Otherwise multiple
masters will appear.

If no new master was promoted in case of problems about syncing, or voting, then
it should be easy for a user to try the promotion again, or even do it
automatically.

There were a couple of standalone promotion implementations, but they have
failed because of 1) complexity, 2) bike reinvention. Promotion was split into
subtasks, each of which could be used even out of promotion.

First was SWIM - failure detection gossip protocol [#3234](https://github.com/tarantool/tarantool/issues/3234).
It is going to deal with failure detection in Raft, replace its original
heartbeats, or at least a part of them. It is one of killer features of
Tarantool Raft - heartbeats put a constant overhead regardless of number of
nodes, and even number of Rafts per one instance.

Second is the subject of the given RFC - Raft. The consensus protocol is going
to deal with uniqueness of promotion decision. It will ensure, that if multiple
instances want to be promoted, only one of them will succeed, and other nodes
will certainly know that. The leader will do promotions. That knowledge is
persisted.

Third and the last is promote procedure itself, which becomes in theory quite
trivial after Raft and SWIM are ready. But it is a subject for another RFC.

#### Out of promotion's scope

Raft is going to be used not only for promotion. Being quite a powerful
algorithm, its presence on board allows to replace some traditional consensus
tools such as Consul, etcd, Zookeeper, etc. To create a totally enclosed
ecosystem.

For example, Raft could be used to select several nodes of a cluster as Raft 
nodes, store on them a configuration of the cluster, and update it
synchronously. Other nodes will read that configuration and be sure, that it is
correct and every other node sees the same configuration.

## Detailed design

Raft in Tarantool is special. It is not a standalone tool, with its own
persistence, networking, and logging. It is a pure implementation of the
algorithm, based totally on the paper's math and virtual functions wherever
something is not related to the algorithm.

For the algorithm details the reader can address the original paper. Here and
below only API and surrounding tools are considered assuming that the reader is
familiar with the algorithm.

#### Persistence

The most basic thing Raft needs is persistence. Raft logs most of actions of
each node, some state details, and of course user requests, and it needs them
recovered on restart.

Raft has a small fixed size persistent state and a list of persisted logs.
State is characterized by current term and id of a candidate to which a vote was
given. List of logs is a list of records like {term = number, key = value}.

The list of logs should be compacted when it gets too big. In the Raft paper it
is called 'snapshot'.

Raft as an algorithm needs this API be implemented externally:

```C
/**
 * A virtual class to provide Raft with persistence. The log is
 * supposed to be able to rotate, be sequential.
 */
struct raft_log {
	/** Virtual functions ... */
};

/**
 * For who the instance has voted. It is used to avoid double vote
 * in case the instance has restarted.
 */
struct raft_log_vote {
	uint64_t term;
	uuid_t candidate;
	/* ... */
};

/**
 * New term number. Can appear solely when, for example, the
 * instance starts leader election, and increments the term
 * without applying any records.
 */
struct raft_log_term {
	uint64_t term;
	/* ... */
};

/**
 * User data. Similar to space tuples: has a primary key by which
 * old records are dropped, and by which a user can read the
 * latest known data.
 */
struct raft_log_entry {
	uint64_t term;
	key_t key;
	data_t data;
	/* ... */
};

/**
 * It is supposed that user is likely to store all the logs in one
 * file. Records in such a file can be distinguished by these
 * identifiers, and Raft expects them when loads log.
 */
enum raft_log_record_type {
	RAFT_LOG_RECORD_VOTE,
	RAFT_LOG_RECORD_TERM,
	RAFT_LOG_RECORD_ENTRY,
};

/** Union of all log types to simplify load API. */
struct raft_log_record {
	enum raft_log_record_type type;
	union {
		struct raft_log_vote vote;
		struct raft_log_term term;
		struct raft_log_entry entry;
	};
};

/** Write a record to the end of log. */

int
raft_log_append_vote(struct raft_log *log, const struct raft_log_vote *e);

int
raft_log_append_term(struct raft_log *log, const struct raft_log_term *e);

int
raft_log_append_entry(struct raft_log *log, const struct raft_log_entry *e);

/**
 * Deletion API. It happens, when a record is logged, but not
 * committed and a new leader is elected, who doesn't even know
 * about that record.
 */
int
raft_log_pop_entry(struct raft_log *log);

/** Load a next record from the log. */

int
raft_log_load_entry(struct raft_log *log, struct raft_log_record *record);

/**
 * Notify that now Raft will do a snapshot. During snapshot Raft
 * will call 'append' function once for each stored key. These
 * appends should be somehow separated from the old log.
 */
int
raft_log_begin_snapshot(struct raft_log *log);

/**
 * Raft signals, that the snapshot is done. All the logs, stored
 * before 'begin_snapshot', should be deleted and never used
 * again.
 */
int
raft_log_end_snapshot(struct raft_log *log);
```

Snapshoting if the most dubious thing here. My idea is that it should be
logically implemented in Raft, but physical implementation is provided by a
user for the functions above.

I omitted some minor things like that end of snapshot should not delete all the
logs, only committed once. Uncommitted logs should be kept even after snapshot.
It is a deal of a couple of parameters, as I understand.

#### Transport

Raft requires the following API implemented externally:

```C
struct raft_node_io {
	/* Socket, address, ... */
};

struct raft_request_append_entries {
	uint64_t term;
	uint32_t log_count;
	struct raft_log_entry *logs;
	/* ... */
};

struct raft_request_vote {
	uint64_t term;
	uuid_t candidate;
	/* ... */
};

int
raft_send_append_entries(struct raft_node_io *io,
			 const struct raft_request_append_entries *req);

int
raft_send_vote(struct raft_node_io *io, const struct raft_request_vote *req);
```

#### Failure detection

Since Raft doesn't have own sockets, it can't detect failures by itself. Raft
uses SWIM for that. A SWIM instance is created outside of Raft, connects to
other Raft nodes, and the SWIM cluster works in a normal mode. Raft subscribes
on SWIM cluster changes. When a node fails, and it is a leader, Raft starts
doing something. For example, tries to choose a new leader.

Possible API to be called by SWIM is quite simple:
```C
void
raft_on_member_update(struct trigger *t, void *event)
{
	/*
	 * Extract member from @a event, and Raft from @a t. If
	 * the member belongs to Raft, then start a new leader
	 * election.
	 */
}
```

#### Behaviour when < 3 nodes

Looks like when there are only 2 nodes available, Raft can't make decisions. But
in fact it can. Unfortunately, for that both nodes should be alive, because
in such a small cluster majority of modes = the whole cluster.

#### Forced election

Sometimes it happens, that something is wrong with the cluster. It is broken due
to an inaccurate configuration, or due to a bug inside an application or
Tarantool. In such a case it is essential to provide a 'root' API for a system
administrator so as he could repair the cluster swiftly.

Since Raft by design does not describe any 'forced' actions, it should be
created solely by Tarantool. It is proposed to introduce a new log record
called, ironic, `promote`. It contains identifier of a leader, a new term
allowed to be set manually, and when an instance receives such a record with a
term > current one, it immediately believes into the new leader.

```C
/** Log record. */
struct raft_log_promote {
	uint64_t term;
	uuid_t leader;
	/* ... */
};

struct raft_request_promote {
	uint64_t term;
	uuid_t leader;
	/* ... */
};

int
raft_log_append_promote(struct raft_log *log, const struct raft_log_promote *e);

int
raft_send_log_promote(struct raft_node_io *io,
		      const struct raft_request_promote *e);

/** Public API for administrators. */
int
raft_promote(uint64_t term);
```

#### Deletion

Raft does not describe how to delete keys permanently. So as they even do not
exist in a next snapshot after deletion. There is a proposal to introduce an
attribute for `struct raft_log_entry` as
`enum {RAFT_LOG_REPLACE, RAFT_LOG_DELETE}`. During snapshot if the latest record
for a key is `RAFT_LOG_DELETE`, then it is totally dropped.

#### Interface

Besides inevitable functions like `raft_new`/`delete`/`add_node`/`remove_node`/
etc, there will be a few core logic functions. They should be generic enough to
be easy to use for any synchronous requests, decisions, reads. A possible
variant is below. Looks like a synchronous simplified space, probably available
before box.cfg. Raft operates by records of type {key, operation, value}. The
API below combines it with box space API.

```C
/**
 * Synchronously publish @a value under @a key. If there are
 * multiple attempts at the same time from other servers to post
 * the same key, then only one of them will succeed. Others will
 * see, that a leader has uncommitted change for that key, and
 * will get an error. In case that key already exists, but is
 * already committed, it is rewritten.
 */
int
raft_replace_key(struct raft *raft, const char *key, const char *value,
		 size_t value_size);

/**
 * The same as replace, but returns an error, if such a key
 * already exists, regardless whether it is committed or not.
 */
int
raft_insert_key(struct raft *raft, const char *key, const char *value,
		size_t value_size);

/** Read the latest committed version of @a key. */
const char *
raft_read_key(struct raft *raft, const char *key, size_t *value_size);

/** Subscribe on commits of all keys. */
void
raft_on_commit(struct raft *raft, struct trigger *t);
```

An example of usage for promotion:
```C
/** Called on a node to promote. */
int
box_ctl_promote(void)
{
	return raft_replace_key(internal_raft, "master", my_uuid, 16);
}

/** Called on a leader. */
void
raft_on_commit(struct raft *raft, struct trigger *t)
{
	if (key == "master") {
		/*
		 * Do promotion. All updates are blocked while
		 * the triggers work.
		 * ...
		 */
	}
}
```

## Rationale and alternatives

#### Persistence

Besides virtual functions implemented externally there were 2 alternatives:

- Use replica local spaces. Replica locality would help to do not mess logs of
  different Raft nodes via built-in replication. Mess is a lack of global source
  of unique sequence numbers, and logs reordering, what was a big pain in the
  ass in the earlier promotion implementations. Raft would have its own
  replication. Replica local spaces would solve the problem of persistence,
  snapshoting, recovery, old xlog files GC. But cons is that Raft wouldn't be
  able to work before box.cfg. Or at least wouldn't be able to vote nor become a
  leader. Only read records into a volatile log list. Moreover, Raft needs fsync
  for each request. In Tarantool there is no way how to fsync individual
  transactions for certain spaces.

- Store logs in a separate xlog file, not a part of WAL. Or even not xlog, but
  with its own structure, very simplified. Pros: Raft becomes totally
  independent from box, can be started before box.cfg. Cons: need to implement
  snapshoting, GC, and how to append to these files - TX thread should not write
  to disk. Probably, coio would be enough for IO.

#### Replication

In the final solution Raft uses virtual transport function of kind
`send_smth()`. There was an alternative with Raft's internal TCP sockets. Pros:
simple, keeps Raft independent. Cons: probably we might want to piggyback Raft
messages via replication sockets, or base Raft on UDP. Dedicated static socket
implementation would not allow this.

#### Existing implementations
